基础知识就像是一座大楼的地基，它决定了我们的技术高度。而要想快速做出点事情，前提条件一定是基础能力过硬，“内功”要到位

越是厉害的公司，越是注重考察数据结构与算法这类基础知识。相比短期能力，他们更看中你的长期潜力。

作为业务开发，我们会用到各种框架、中间件和底层系统，比如 Spring、RPC 框架、消息中间件、Redis 等等。在这些基础框架中，一般都揉和了很多基础数据结构和算法的设计思想。

基础架构研发工程师，写出达到开源水平的框架才是你的目标！

高手之间的竞争其实就在细节。这些细节包括：你用的算法是不是够优化，数据存取的效率是不是够高，内存是不是够节省等等。这些累积起来，决定了一个框架是不是优秀。

何为编程能力强？是代码的可读性好、健壮？还是扩展性好？我觉得没法列，也列不全。但是，在我看来，性能好坏起码是其中一个非常重要的评判标准。

数据结构和算法是相辅相成的。数据结构是为算法服务的，算法要作用在特定的数据结构之上。

想要学习数据结构与算法，首先要掌握一个数据结构与算法中最重要的概念——复杂度分析。
数据结构和算法解决的是如何更省、更快地存储和处理数据的问题

学习要学会找重点

20 个最常用的、最基础数据结构与算法，不管是应付面试还是工作需要，只要集中精力逐一攻克这 20 个知识点就足够了。
这里面有 10 个数据结构：数组、链表、栈、队列、散列表、二叉树、堆、跳表、图、Trie 树；10 个算法：递归、排序、二分查找、搜索、哈希算法、贪心算法、分治算法、回溯算法、动态规划、字符串匹配算法。

在学习数据结构和算法的过程中，要学习它的“来历”“自身的特点”“适合解决的问题”以及“实际的应用场景”。
学习知识的过程是反复迭代、不断沉淀的过程。

只要讲到数据结构与算法，就一定离不开时间、空间复杂度分析

事后统计法:
1. 测试结果非常依赖测试环境
2. 测试结果受数据规模的影响很大

我们需要一个不用具体的测试数据来测试，就可以粗略地估计算法的执行效率的方法

所有代码的执行时间 T(n) 与每行代码的执行次数n成正比

	T(n) = O(f(n))

f(n) 表示每行代码执行的次数总和
公式中的O，表示代码的执行时间 T(n) 与 f(n) 表达式成正比。

大O时间复杂度实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随数据规模增长的变化趋势，所以，也叫作渐进时间复杂度（asymptotic time complexity），简称时间复杂度。

当 n 很大时，你可以把它想象成10000、100000。而公式中的低阶、常量、系数三部分并不左右增长趋势，所以都可以忽略。

- O(1)

- O(log n), O(nLog n)

		i = 1;
		while ( i <= n) {
			i = i * 2;
		}

log3n 就等于 log32 * log2n，所以 O(log3n) = O(C * log2n)，其中 C=log32 是一个常量。基于我们前面的一个理论：在采用大 O 标记复杂度的时候，可以忽略系数，即 O(Cf(n)) = O(f(n))。

- O(m+n), O(m*n)


时间复杂度的全称是渐进时间复杂度，表示算法的执行时间与数据规模之间的增长关系。
类比一下，空间复杂度全称就是渐进空间复杂度（asymptotic space complexity），表示算法的存储空间与数据规模之间的增长关系。



最好情况时间复杂度（best case time complexity）、
最坏情况时间复杂度（worst case time complexity）、
平均情况时间复杂度（average case time complexity）、
均摊时间复杂度（amortized time complexity）




数组

我们拿一个长度为 10 的 int 类型的数组 int[] a = new int[10] 来举例。例如计算机给数组 a[10]，分配了一块连续内存空间 1000～1039，其中，内存块的首地址为 base_address = 1000。

	a[i]_address = base_address + i * data_type_size


每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除。当数组没有更多空间存储数据时，我们再触发执行一次真正的删除操作，这样就大大减少了删除操作导致的数据搬移。



链表
一个经典的链表应用场景，那就是 LRU 缓存淘汰算法
缓存是一种提高数据读取性能的技术
缓存的大小有限，当缓存被用满时，哪些数据应该被清理出去，哪些数据应该被保留？这就需要缓存淘汰策略来决定。常见的策略有三种：先进先出策略 FIFO（First In，First Out）、最少使用策略 LFU（Least Frequently Used）、最近最少使用策略 LRU（Least Recently Used）。

单链表、双向链表和循环链表

双向链表需要额外的两个空间来存储后继结点和前驱结点的地址。所以，如果存储同样多的数据，双向链表要比单链表占用更多的内存空间。虽然两个指针比较浪费存储空间，但可以支持双向遍历，这样也带来了双向链表操作的灵活性。
从结构上来看，双向链表可以支持 O(1) 时间复杂度的情况下找到前驱结点

尽管单纯的删除操作时间复杂度是 O(1)，但遍历查找的时间是主要的耗时点，对应的时间复杂度为 O(n)。

用空间换时间的设计思想

当内存空间充足的时候，如果我们更加追求代码的执行速度，我们就可以选择空间复杂度相对较高、但时间复杂度相对很低的算法或者数据结构。相反，如果内存比较紧缺，比如代码跑在手机或者单片机上，这个时候，就要反过来用时间换空间的设计思路。

数组简单易用，在实现上使用的是连续的内存空间，可以借助CPU的缓存机制，预读数组中的数据，所以访问效率更高。
而链表在内存中并不是连续存储，所以对 CPU 缓存不友好，没办法有效预读。

数组的缺点是大小固定，一经声明就要占用整块连续内存空间。如果声明的数组过大，系统可能没有足够的连续内存空间分配给它，导致“内存不足（out of memory）”。如果声明的数组过小，则可能出现不够用的情况。这时只能再申请一个更大的内存空间，把原数组拷贝进去，非常费时。链表本身没有大小的限制，天然地支持动态扩容，我觉得这也是它与数组最大的区别。




栈

后进者先出，先进者后出，这就是典型的“栈”结构

用数组实现的栈，我们叫作顺序栈，用链表实现的栈，我们叫作链式栈。




队列

public boolean enqueue(String item) {
    // 
    if (tail == n) {
        if (head == 0) return false;

        for (int i = head; i < tail; ++i) {
            items[i-head] = items[i];
        }

        tail -= head;
        head = 0;
    }

    items[tail] = item;
    ++tail;

    return true;
}

在用数组实现的非循环队列中，队满的判断条件是 tail == n，队空的判断条件是 head == tail。
针对循环队列,队列为空的判断条件仍然是 head == tail。当队满时，(tail+1)%n=head
当队列满时，tail指向的位置实际上是没有存储数据的。所以，循环队列会浪费一个数组的存储空间。


阻塞队列和并发队列
阻塞队列其实就是在队列基础上增加了阻塞操作。简单来说，就是在队列为空的时候，从队头取数据会被阻塞。因为此时还没有数据可取，直到队列中有了数据才能返回；如果队列已经满了，那么插入数据的操作就会被阻塞，直到队列中有空闲位置后再插入数据，然后再返回。

线程安全的队列我们叫作并发队列。最简单直接的实现方式是直接在 enqueue()、dequeue() 方法上加锁，但是锁粒度大并发度会比较低，同一时刻仅允许一个存或者取操作。实际上，基于数组的循环队列，利用 CAS 原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用更加广泛的原因。

高性能队列 Disruptor、Linux 环形缓存，都用到了循环并发队列；Java concurrent 并发包利用 ArrayBlockingQueue 来实现公平锁等。




递归
假如这里有 n 个台阶，每次你可以跨 1 个台阶或者 2 个台阶，请问走这 n 个台阶有多少种走法？
实际上，可以根据第一步的走法把所有走法分为两类，第一类是第一步走了 1 个台阶，另一类是第一步走了 2 个台阶。
所以 n 个台阶的走法就等于先走 1 阶后，n-1 个台阶的走法 加上先走 2 阶后，n-2 个台阶的走法。用公式表示就是：
    f(n) = f(n-1)+f(n-2)
有了递推公式，递归代码基本上就完成了一半。我们再来看下终止条件。
我们可以用 n=2，n=3 这样比较小的数试验一下。
递归终止条件就是 f(1)=1，f(2)=2。
    f(1) = 1;
    f(2) = 2;
    f(n) = f(n-1)+f(n-2)
最终代码：

    def f(n: Int): Int = n match {
        case y if y < 1 => throw new Exception()
        case 1 => 1
        case 2 => 2
        case x => f(x-1) + f(x-2)
    }

写递归代码的关键就是找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码。

递归代码要警惕堆栈溢出
函数调用会使用栈来保存临时变量。每调用一个函数，都会将临时变量封装为栈帧压入内存栈，等函数执行完成返回时，才出栈。系统栈或者虚拟机栈空间一般都不大。如果递归求解的数据规模很大，调用层次很深，一直压入栈，就会有堆栈溢出的风险。

除了堆栈溢出、重复计算这两个常见的问题。递归代码还有很多别的问题。
在时间效率上，递归代码里多了很多函数调用，当这些函数调用的数量较大时，就会积聚成一个可观的时间成本。在空间复杂度上，因为递归调用一次就会在内存栈中保存一次现场数据，所以在分析递归代码空间复杂度时，需要额外考虑这部分的开销，比如我们前面讲到的电影院递归代码，空间复杂度并不是 O(1)，而是 O(n)。



排序
最经典的、最常用的：冒泡排序、插入排序、选择排序、归并排序、快速排序、计数排序、基数排序、桶排序。

对于排序算法执行效率的分析，我们一般会从这几个方面来衡量：
1. 最好情况、最坏情况、平均情况时间复杂度
2. 时间复杂度的系数、常数 、低阶
实际的软件开发中，我们排序的可能是 10 个、100 个、1000 个这样规模很小的数据，所以，在对同一阶时间复杂度的排序算法性能对比的时候，我们就要把系数、常数、低阶也考虑进来。
3. 比较次数和交换（或移动）次数
基于比较的排序算法的执行过程，会涉及两种操作，一种是元素比较大小，另一种是元素交换或移动。所以，如果我们在分析排序算法的执行效率的时候，应该把比较次数和交换（或移动）次数也考虑进去。

排序算法的内存消耗
算法的内存消耗可以通过空间复杂度来衡量
原地排序算法，就是特指空间复杂度是 O(1) 的排序算法。

排序算法的稳定性
稳定性。这个概念是说，如果待排序的序列中存在值相等的元素，经过排序之后，相等元素之间原有的先后顺序不变。

如果我们现在有10万条订单数据，我们希望按照金额从小到大对订单数据排序。对于金额相同的订单，我们希望按照下单时间从早到晚有序。

稳定排序算法可以保持金额相同的两个对象，在排序之后的前后顺序不变。



冒泡排序（Bubble Sort）

void bubbleSort(int[] a, int n) {
    if (n <= 1) return;

    for (int i = 0; i < n; ++i) {
        boolean flag = false;   // 提前退出冒泡循环的标志位

        for (int j = 0; j < n - i - 1; ++j) {
            if (a[j] > a[j+1]) {
                int tmp = a[j];
                a[j] = a[j+1];
                a[j+1]=tmp;

                flag = true;    // 有数据交换
            }
        }

        if (!flag) break; 
    }
}



插入排序（Insertion Sort）
首先，我们将数组中的数据分为两个区间，已排序区间和未排序区间。初始已排序区间只有一个元素，就是数组的第一个元素。插入算法的核心思想是取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序。重复这个过程，直到未排序区间中元素为空，算法结束。
void insertionSort(int[] a, int n) {
    if (n < 1) return;

    for (int i = 1; i < n; ++i) {
        int value = a[i];

        int j = i - 1;  //  被比较元素的前一个元素的索引

        // 查找插入的位置
        for(; j >= 0; --j) {
            if (a[j] > value) {
                a[j+1] = a[j];
            } else {
                break;
            }
        }
        a[j+1] = value;
    }
}



选择排序（Selection Sort）
选择排序每次会从未排序区间中找到最小的元素，将其放到已排序区间的末尾。



归并排序和快速排序都用到了分治思想，非常巧妙。我们可以借鉴这个思想，来解决非排序的问题，比如：如何在 O(n) 的时间复杂度内查找一个无序数组中的第 K 大元素？

归并排序
归并排序的核心思想还是蛮简单的。如果要排序一个数组，我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。

分治是一种解决问题的处理思想，递归是一种编程技巧

递推公式：
merge_sort(p…r) = merge(merge_sort(p…q), merge_sort(q+1…r))
终止条件：
p >= r 不用再继续分解


merge_sort(p...r) = merge(merge_sort(p...q), merge_sort(q+1...r))

merge_sort(A, n) {
    merge_sort_c(A, 0, n-1);
}

merge_sort_c(A, p, r) {
    if (p >= r) then return

    q = (p + r) / 2
    merge_sort_c(A, p, q)
    merge_sort_c(A, q+1, r)

    merge(A[p...r], A[p...q], A[q+1...r])
}

merge(A[p...r], A[p...q], A[q+1...r]) {
  var i := p，j := q+1，k := 0 // 初始化变量i, j, k
  var tmp := new array[0...r-p] // 申请一个大小跟A[p...r]一样的临时数组
  while i<=q AND j<=r do {
    if A[i] <= A[j] {
      tmp[k++] = A[i++] // i++等于i:=i+1
    } else {
      tmp[k++] = A[j++]
    }
  }
  
  // 判断哪个子数组中有剩余的数据
  var start := i，end := q
  if j<=r then start := j, end:=r
  
  // 将剩余的数据拷贝到临时数组tmp
  while start <= end do {
    tmp[k++] = A[start++]
  }
  
  // 将tmp中的数组拷贝回A[p...r]
  for i:=0 to r-p do {
    A[p+i] = tmp[i]
  }
}


快速排序的原理
快排的思想是这样的：如果要排序数组中下标从 p 到 r 之间的一组数据，我们选择 p 到 r 之间的任意一个数据作为 pivot（分区点）。我们遍历 p 到 r 之间的数据，将小于 pivot 的放到左边，将大于 pivot 的放到右边，将 pivot 放到中间。经过这一步骤之后，数组 p 到 r 之间的数据就被分成了三个部分，前面 p 到 q-1 之间都是小于 pivot 的，中间是 pivot，后面的 q+1 到 r 之间是大于 pivot 的。


递推公式：   quick_sort(p…r) = quick_sort(p…q-1) + quick_sort(q+1… r)
终止条件：   p >= r


// 快速排序，A是数组，n表示数组的大小
quick_sort(A, n) {
  quick_sort_c(A, 0, n-1)
}
// 快速排序递归函数，p,r为下标
quick_sort_c(A, p, r) {
  if p >= r then return
  
  q = partition(A, p, r) // 获取分区点
  quick_sort_c(A, p, q-1)
  quick_sort_c(A, q+1, r)
}


partition(A, p, r) {
  pivot := A[r]
  i := p
  for j := p to r-1 do {
    if A[j] < pivot {
      swap A[i] with A[j]
      i := i+1
    }
  }
  swap A[i] with A[r]
  return i










Redis 中，键的数据类型是字符串,值的数据类型有很多，常用的数据类型有这样几种，它们分别是字符串、列表、字典、集合、有序集合。



